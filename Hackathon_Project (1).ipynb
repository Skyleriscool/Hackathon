{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-multilearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBo3KjAymRyW",
        "outputId": "753e3eff-69b3-4720-aa66-8f6fef6a611c"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-multilearn in /usr/local/lib/python3.12/dist-packages (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "kjKKuYQj8Vm9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from skmultilearn.adapt import MLkNN\n",
        "from sklearn.metrics import hamming_loss, accuracy_score\n",
        "from sklearn.neighbors import NearestNeighbors"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def parse_tag_string(tag_string):\n",
        "    if pd.isna(tag_string) or tag_string == \"\":\n",
        "        return []\n",
        "\n",
        "    # split by '+'\n",
        "    parts = tag_string.split(\"+\")\n",
        "\n",
        "    # keep only valid numeric IDs\n",
        "    cleaned = [p.strip() for p in parts if p.strip().isdigit()]\n",
        "\n",
        "    return cleaned"
      ],
      "metadata": {
        "id": "im3rByxILDCO"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_mknn(X, Y, k=5):\n",
        "    \"\"\"\n",
        "    X: np.ndarray (num_works, num_features)\n",
        "    Y: np.ndarray (num_works, num_tags) â€” each row is 0/1 per tag\n",
        "    \"\"\"\n",
        "    knn = NearestNeighbors(n_neighbors=k, metric=\"cosine\")\n",
        "    knn.fit(X)\n",
        "    return knn, Y\n",
        "\n",
        "\n",
        "def predict_tags_mknn(knn, Y, word_count_vector, tags_work, threshold=0.2):\n",
        "    \"\"\"\n",
        "    knn: fitted NearestNeighbors model\n",
        "    Y: label matrix from training (num_works, num_tags)\n",
        "    word_count_vector: np.ndarray (num_features,)\n",
        "    tags_work: list of strings, tag names\n",
        "    threshold: minimum probability for a tag to be predicted\n",
        "    \"\"\"\n",
        "\n",
        "    # Find nearest neighbors\n",
        "    distances, indices = knn.kneighbors([word_count_vector])\n",
        "\n",
        "    # Gather neighbor tag vectors\n",
        "    neighbor_tags = Y[indices[0]]  # shape: (k, num_tags)\n",
        "\n",
        "    # Compute average label presence (freq among k neighbors)\n",
        "    tag_scores = neighbor_tags.mean(axis=0)\n",
        "\n",
        "    # Choose tags above threshold\n",
        "    predicted = [\n",
        "        tags_work[i]\n",
        "        for i, score in enumerate(tag_scores)\n",
        "        if score >= threshold\n",
        "    ]\n",
        "\n",
        "    return predicted, tag_scores"
      ],
      "metadata": {
        "id": "EucvWJVt13Bh"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Description: This will be an AI project focused on generating potential tags for AO3 based fanfiction\n",
        "aspects_df = pd.read_csv('tags-20210226.csv')\n",
        "aspects_df['name'] = aspects_df['name'].astype(str)\n",
        "id = aspects_df[\"id\"]\n",
        "tags_name = aspects_df[\"name\"]\n",
        "aspects_df2 = pd.read_csv('works-20210226.csv')\n",
        "aspects_df2[\"tag_id_list\"] = aspects_df2[\"tags\"].apply(parse_tag_string)\n",
        "tag_id_to_name = dict(zip(id.astype(str), tags_name))\n",
        "def ids_to_names(id_list):\n",
        "    return [tag_id_to_name[i] for i in id_list if i in tag_id_to_name]\n",
        "\n",
        "aspects_df2[\"tag_name_list\"] = aspects_df2[\"tag_id_list\"].apply(ids_to_names)\n",
        "\n",
        "aspects_df2[\"tag_name_list\"] = aspects_df2[\"tag_name_list\"].apply(lambda lst: \",\".join(lst))\n",
        "tags_work = aspects_df2[\"tag_name_list\"]\n",
        "#tags_work = tags_work.astype(\"string\")\n",
        "word_count = aspects_df2[\"word_count\"]\n",
        "#word_count = word_count.astype(\"string\")\n",
        "print(tags_work)"
      ],
      "metadata": {
        "id": "1TORpuhx9ZbG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7b4b61c-4361-44f4-a953-eb5770c24ce8"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0                    Explicit,M/M,No Archive Warnings Apply\n",
            "1         Explicit,Dubious Consent,Rimming,Dealfic,M/M,N...\n",
            "2         Explicit,Star Trek,Star Trek: The Original Ser...\n",
            "3         Avatar: The Last Airbender,Zuko (Avatar),Pinin...\n",
            "4         Teen And Up Audiences,F/M,Gen,Graphic Depictio...\n",
            "                                ...                        \n",
            "143402    Explicit,Alternate Universe - Modern Setting,C...\n",
            "143403    General Audiences,Supernatural,Sam Winchester,...\n",
            "143404    Teen And Up Audiences,Angst,Gen,No Archive War...\n",
            "143405    Mature,One Piece,Nico Robin,Roronoa Zoro,Origi...\n",
            "143406    Teen And Up Audiences,Naruto,Uchiha Itachi,Alt...\n",
            "Name: tag_name_list, Length: 143407, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "# Create a multi-label binarizer for tags\n",
        "mlb = MultiLabelBinarizer()\n",
        "y_tags = mlb.fit_transform([tags.split(\",\") for tags in tags_work])\n",
        "\n",
        "# Use word_count as features (or use TF-IDF of tag text descriptions if available)\n",
        "# For now, we'll create a simple feature from word_count\n",
        "X_features = np.column_stack([word_count.values])  # Can add more features here\n",
        "\n",
        "# Split the data\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_features, y_tags, test_size=0.30, random_state=42)\n"
      ],
      "metadata": {
        "id": "zoG1aqZGoAy_"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlknn_classifier = MLkNN()\n",
        "mlknn_classifier.fit(x_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "upvuvmOwrYhy",
        "outputId": "f1118fe4-c6f2-4f22-b75c-e9e1f73d09e4"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "NearestNeighbors.__init__() takes 1 positional argument but 2 were given",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2515921732.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmlknn_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLkNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmlknn_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/skmultilearn/adapt/mlknn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prior_prob_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prior_prob_false\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_label_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;31m# Computing the posterior probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond_prob_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond_prob_false\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_cond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_label_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/skmultilearn/adapt/mlknn.py\u001b[0m in \u001b[0;36m_compute_cond\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \"\"\"\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mknn_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNearestNeighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlil_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'i8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mcn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlil_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'i8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: NearestNeighbors.__init__() takes 1 positional argument but 2 were given"
          ]
        }
      ]
    }
  ]
}