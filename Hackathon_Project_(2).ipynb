{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-multilearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZBo3KjAymRyW",
        "outputId": "d8e6ff18-095f-43dc-f7b2-d01de6565fcc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-multilearn in /usr/local/lib/python3.12/dist-packages (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_G1PZMeBfzGd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "kjKKuYQj8Vm9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from skmultilearn.adapt import MLkNN\n",
        "from sklearn.metrics import hamming_loss, accuracy_score\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility: parse the tag id string from works CSV into a list of tag id strings\n",
        "import pandas as _pd\n",
        "\n",
        "def parse_tag_string(s):\n",
        "    if _pd.isna(s):\n",
        "        return []\n",
        "    parts = [p for p in str(s).split('+') if p != '']\n",
        "    return parts\n",
        "\n",
        "# Optional helper to inspect NearestNeighbors (debugging shadowing issues)\n",
        "def debug_nearestneighbors():\n",
        "    print('scikit-learn version:', sklearn.__version__)\n",
        "    print('scikit-multilearn version:', skmultilearn.__version__)\n",
        "    print('NearestNeighbors object:', NearestNeighbors)\n",
        "    try:\n",
        "        print('NearestNeighbors.__init__ signature:', inspect.signature(NearestNeighbors.__init__))\n",
        "    except Exception as e:\n",
        "        print('Could not get signature:', e)"
      ],
      "metadata": {
        "id": "im3rByxILDCO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_mknn(X, Y, k=5):\n",
        "    \"\"\"\n",
        "    X: np.ndarray (num_works, num_features)\n",
        "    Y: np.ndarray (num_works, num_tags) â€” each row is 0/1 per tag\n",
        "    \"\"\"\n",
        "    knn = NearestNeighbors(n_neighbors=k, metric=\"cosine\")\n",
        "    knn.fit(X)\n",
        "    return knn, Y\n",
        "\n",
        "\n",
        "def predict_tags_mknn(knn, Y, word_count_vector, tags_work, threshold=0.2):\n",
        "    \"\"\"\n",
        "    knn: fitted NearestNeighbors model\n",
        "    Y: label matrix from training (num_works, num_tags)\n",
        "    word_count_vector: np.ndarray (num_features,)\n",
        "    tags_work: list of strings, tag names\n",
        "    threshold: minimum probability for a tag to be predicted\n",
        "    \"\"\"\n",
        "\n",
        "    # Find nearest neighbors\n",
        "    distances, indices = knn.kneighbors([word_count_vector])\n",
        "\n",
        "    # Gather neighbor tag vectors\n",
        "    neighbor_tags = Y[indices[0]]  # shape: (k, num_tags)\n",
        "\n",
        "    # Compute average label presence (freq among k neighbors)\n",
        "    tag_scores = neighbor_tags.mean(axis=0)\n",
        "\n",
        "    # Choose tags above threshold\n",
        "    predicted = [\n",
        "        tags_work[i]\n",
        "        for i, score in enumerate(tag_scores)\n",
        "        if score >= threshold\n",
        "    ]\n",
        "\n",
        "    return predicted, tag_scores"
      ],
      "metadata": {
        "id": "EucvWJVt13Bh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Description: This will be an AI project focused on generating potential tags for AO3 based fanfiction\n",
        "aspects_df = pd.read_csv('tags-20210226.csv')\n",
        "aspects_df['name'] = aspects_df['name'].astype(str)\n",
        "id = aspects_df[\"id\"]\n",
        "tags_name = aspects_df[\"name\"]\n",
        "aspects_df2 = pd.read_csv('works-20210226.csv')\n",
        "aspects_df2[\"tag_id_list\"] = aspects_df2[\"tags\"].apply(parse_tag_string)\n",
        "tag_id_to_name = dict(zip(id.astype(str), tags_name))\n",
        "def ids_to_names(id_list):\n",
        "    return [tag_id_to_name[i] for i in id_list if i in tag_id_to_name]\n",
        "\n",
        "aspects_df2[\"tag_name_list\"] = aspects_df2[\"tag_id_list\"].apply(ids_to_names)\n",
        "\n",
        "aspects_df2[\"tag_name_list\"] = aspects_df2[\"tag_name_list\"].apply(lambda lst: \",\".join(lst))\n",
        "tags_work = aspects_df2[\"tag_name_list\"]\n",
        "#tags_work = tags_work.astype(\"string\")\n",
        "word_count = aspects_df2[\"word_count\"]\n",
        "#word_count = word_count.astype(\"string\")\n",
        "print(tags_work)"
      ],
      "metadata": {
        "id": "1TORpuhx9ZbG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0129642-1c41-47ac-fb4a-a657d137cf29"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0                 Explicit,M/M,No Archive Warnings Apply\n",
            "1      Explicit,Dubious Consent,Rimming,Dealfic,M/M,N...\n",
            "2      Explicit,Star Trek,Star Trek: The Original Ser...\n",
            "3      Avatar: The Last Airbender,Alternate Universe,...\n",
            "4      Teen And Up Audiences,F/M,Gen,Graphic Depictio...\n",
            "                             ...                        \n",
            "994    Teen And Up Audiences,Hurt/Comfort,M/M,Choose ...\n",
            "995    Dream,Gen,Graphic Depictions Of Violence,Teen ...\n",
            "996    Teen And Up Audiences,Dream,M/M,Choose Not To ...\n",
            "997    Teen And Up Audiences,Dream,Gen,Graphic Depict...\n",
            "998    Mature,Hurt/Comfort,Romance,M/M,No Archive War...\n",
            "Name: tag_name_list, Length: 999, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "# Create a multi-label binarizer for tags\n",
        "mlb = MultiLabelBinarizer()\n",
        "y_tags = mlb.fit_transform([tags.split(\",\") for tags in tags_work])\n",
        "\n",
        "# Use word_count as features (or use TF-IDF of tag text descriptions if available)\n",
        "# For now, we'll create a simple feature from word_count\n",
        "X_features = np.column_stack([word_count.values])  # Can add more features here\n",
        "\n",
        "# Split the data\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_features, y_tags, test_size=0.30, random_state=42)\n"
      ],
      "metadata": {
        "id": "zoG1aqZGoAy_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Debug check to detect shadowing or incompatible versions\n",
        "#debug_nearestneighbors()\n",
        "\n",
        "# Train MLkNN - ensure inputs are the right shapes and types\n",
        "mlknn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
        "mlknn_classifier.fit(x_train, y_train)\n",
        "# Predict on test set and show a simple metric\n",
        "y_pred = mlknn_classifier.predict(x_test)\n",
        "print('Hamming loss:', hamming_loss(y_test, y_pred))"
      ],
      "metadata": {
        "id": "upvuvmOwrYhy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c37dc22-a587-4629-bbdf-86e69b8904b4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hamming loss: 0.016994871794871795\n"
          ]
        }
      ]
    }
  ]
}