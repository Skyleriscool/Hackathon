{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-multilearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZBo3KjAymRyW",
        "outputId": "49326121-c49a-4288-8d10-33add363092a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-multilearn in /usr/local/lib/python3.12/dist-packages (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_G1PZMeBfzGd"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "kjKKuYQj8Vm9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from skmultilearn.adapt import MLkNN\n",
        "from sklearn.metrics import hamming_loss, accuracy_score\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility: parse the tag id string from works CSV into a list of tag id strings\n",
        "import pandas as _pd\n",
        "\n",
        "def parse_tag_string(s):\n",
        "    if _pd.isna(s):\n",
        "        return []\n",
        "    parts = [p for p in str(s).split('+') if p != '']\n",
        "    return parts\n",
        "\n",
        "# Optional helper to inspect NearestNeighbors (debugging shadowing issues)\n",
        "def debug_nearestneighbors():\n",
        "    print('scikit-learn version:', sklearn.__version__)\n",
        "    print('scikit-multilearn version:', skmultilearn.__version__)\n",
        "    print('NearestNeighbors object:', NearestNeighbors)\n",
        "    try:\n",
        "        print('NearestNeighbors.__init__ signature:', inspect.signature(NearestNeighbors.__init__))\n",
        "    except Exception as e:\n",
        "        print('Could not get signature:', e)"
      ],
      "metadata": {
        "id": "im3rByxILDCO"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_mknn(X, Y, k=5):\n",
        "    \"\"\"\n",
        "    X: np.ndarray (num_works, num_features)\n",
        "    Y: np.ndarray (num_works, num_tags) — each row is 0/1 per tag\n",
        "    \"\"\"\n",
        "    knn = NearestNeighbors(n_neighbors=k, metric=\"cosine\")\n",
        "    knn.fit(X)\n",
        "    return knn, Y\n",
        "\n",
        "\n",
        "def predict_tags_mknn(knn, Y, word_count_vector, tags_work, threshold=0.2):\n",
        "    \"\"\"\n",
        "    knn: fitted NearestNeighbors model\n",
        "    Y: label matrix from training (num_works, num_tags)\n",
        "    word_count_vector: np.ndarray (num_features,)\n",
        "    tags_work: list of strings, tag names\n",
        "    threshold: minimum probability for a tag to be predicted\n",
        "    \"\"\"\n",
        "\n",
        "    # Find nearest neighbors\n",
        "    distances, indices = knn.kneighbors([word_count_vector])\n",
        "\n",
        "    # Gather neighbor tag vectors\n",
        "    neighbor_tags = Y[indices[0]]  # shape: (k, num_tags)\n",
        "\n",
        "    # Compute average label presence (freq among k neighbors)\n",
        "    tag_scores = neighbor_tags.mean(axis=0)\n",
        "\n",
        "    # Choose tags above threshold\n",
        "    predicted = [\n",
        "        tags_work[i]\n",
        "        for i, score in enumerate(tag_scores)\n",
        "        if score >= threshold\n",
        "    ]\n",
        "\n",
        "    return predicted, tag_scores"
      ],
      "metadata": {
        "id": "EucvWJVt13Bh"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Description: This will be an AI project focused on generating potential tags for AO3 based fanfiction\n",
        "aspects_df = pd.read_csv('tags-20210226.csv')\n",
        "aspects_df['name'] = aspects_df['name'].astype(str)\n",
        "id = aspects_df[\"id\"]\n",
        "tags_name = aspects_df[\"name\"]\n",
        "aspects_df2 = pd.read_csv('works-20210226.csv')\n",
        "aspects_df2[\"tag_id_list\"] = aspects_df2[\"tags\"].apply(parse_tag_string)\n",
        "tag_id_to_name = dict(zip(id.astype(str), tags_name))\n",
        "def ids_to_names(id_list):\n",
        "    return [tag_id_to_name[i] for i in id_list if i in tag_id_to_name]\n",
        "\n",
        "aspects_df2[\"tag_name_list\"] = aspects_df2[\"tag_id_list\"].apply(ids_to_names)\n",
        "\n",
        "aspects_df2[\"tag_name_list\"] = aspects_df2[\"tag_name_list\"].apply(lambda lst: \",\".join(lst))\n",
        "tags_work = aspects_df2[\"tag_name_list\"]\n",
        "#tags_work = tags_work.astype(\"string\")\n",
        "word_count = aspects_df2[\"word_count\"]\n",
        "#word_count = word_count.astype(\"string\")\n",
        "print(tags_work)"
      ],
      "metadata": {
        "id": "1TORpuhx9ZbG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83ee88ce-ad1e-451a-d686-f9680a59952f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0                 Explicit,M/M,No Archive Warnings Apply\n",
            "1      Explicit,Dubious Consent,Rimming,Dealfic,M/M,N...\n",
            "2      Explicit,Star Trek,Star Trek: The Original Ser...\n",
            "3      Avatar: The Last Airbender,Alternate Universe,...\n",
            "4      Teen And Up Audiences,F/M,Gen,Graphic Depictio...\n",
            "                             ...                        \n",
            "994    Teen And Up Audiences,Hurt/Comfort,M/M,Choose ...\n",
            "995    Dream,Gen,Graphic Depictions Of Violence,Teen ...\n",
            "996    Teen And Up Audiences,Dream,M/M,Choose Not To ...\n",
            "997    Teen And Up Audiences,Dream,Gen,Graphic Depict...\n",
            "998    Mature,Hurt/Comfort,Romance,M/M,No Archive War...\n",
            "Name: tag_name_list, Length: 999, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "# Create a multi-label binarizer for tags\n",
        "mlb = MultiLabelBinarizer()\n",
        "y_tags = mlb.fit_transform([tags.split(\",\") for tags in tags_work])\n",
        "\n",
        "# Use word_count as features (or use TF-IDF of tag text descriptions if available)\n",
        "# For now, we'll create a simple feature from word_count\n",
        "X_features = np.column_stack([word_count.values])  # Can add more features here\n",
        "\n",
        "# Split the data\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_features, y_tags, test_size=0.30, random_state=42)\n"
      ],
      "metadata": {
        "id": "zoG1aqZGoAy_"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Debug check to detect shadowing or incompatible versions\n",
        "#debug_nearestneighbors()\n",
        "\n",
        "# Train MLkNN - ensure inputs are the right shapes and types\n",
        "mlknn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
        "mlknn_classifier.fit(x_train, y_train)\n",
        "# Predict on test set and show a simple metric\n",
        "y_pred = mlknn_classifier.predict(x_test)\n",
        "print('Hamming loss:', hamming_loss(y_test, y_pred))"
      ],
      "metadata": {
        "id": "upvuvmOwrYhy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "239968cc-1c1f-4c88-f276-0d02b691d2be"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hamming loss: 0.016994871794871795\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas scikit-learn gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wR5KBz71hMNl",
        "outputId": "3728e340-a579-4402-efe7-70e78218107b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.49.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.11.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.2.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.121.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (1.0.0)\n",
            "Requirement already satisfied: gradio-client==1.13.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.13.3)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.36.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.11.10)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.4)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.49.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.20.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.38.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.4)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "import gradio as gr\n",
        "\n",
        "# ---------------------------\n",
        "# Utility / Preprocessing\n",
        "# ---------------------------\n",
        "def parse_tag_string(tag_string):\n",
        "    \"\"\"Convert AO3 style '123+45+9001' into ['123','45','9001'] safely.\"\"\"\n",
        "    if pd.isna(tag_string) or tag_string == \"\":\n",
        "        return []\n",
        "    parts = str(tag_string).split(\"+\")\n",
        "    cleaned = [p.strip() for p in parts if p and p.strip().isdigit()]\n",
        "    return cleaned\n",
        "\n",
        "def build_tag_mapping(tags_df, id_col=None, name_col=None):\n",
        "    \"\"\"Return mapping str(id)->name from tags dataframe. Attempts common column names.\"\"\"\n",
        "    if id_col is None:\n",
        "        for c in tags_df.columns:\n",
        "            if \"id\" in c.lower():\n",
        "                id_col = c\n",
        "                break\n",
        "    if name_col is None:\n",
        "        for c in tags_df.columns:\n",
        "            if \"name\" in c.lower() or \"tag\" in c.lower():\n",
        "                name_col = c\n",
        "                break\n",
        "    if id_col is None or name_col is None:\n",
        "        raise ValueError(\"Couldn't auto-detect id/name columns in tags file. Please ensure columns contain 'id' and 'name' words.\")\n",
        "    mapping = {str(r): n for r, n in zip(tags_df[id_col].astype(str), tags_df[name_col].astype(str))}\n",
        "    return mapping, id_col, name_col\n",
        "\n",
        "def ids_to_names(id_list, mapping):\n",
        "    return [mapping[i] for i in id_list if i in mapping]\n",
        "\n",
        "def names_list_to_csv_string(name_list):\n",
        "    if not name_list:\n",
        "        return \"\"\n",
        "    # Join with commas, ensure dtype str on output\n",
        "    return \",\".join([str(x).strip() for x in name_list])\n",
        "\n",
        "# ---------------------------\n",
        "# Model helper: lightweight baseline\n",
        "# ---------------------------\n",
        "class TagPredictorBaseline:\n",
        "    \"\"\"\n",
        "    TF-IDF on a concatenated metadata text + One-vs-Rest Logistic Regression.\n",
        "    This is a fast baseline suitable for the notebook UI/prototype.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.mlb = None\n",
        "        self.pipeline = None\n",
        "        self.tag_index_to_name = None\n",
        "\n",
        "    def build_input_text(self, df):\n",
        "        # Choose a set of metadata columns to summarize into text\n",
        "        # Use many columns if available to give the model some signal.\n",
        "        cols = [c for c in df.columns if c.lower() not in (\"tags\",\"tag_id_list\",\"tag_name_list\")]\n",
        "        # create text by concatenating column:value tokens\n",
        "        text_series = df[cols].fillna(\"\").astype(str).apply(lambda row: \" \".join([f\"{col}:{row[col]}\" for col in cols]), axis=1)\n",
        "        return text_series\n",
        "\n",
        "    def fit(self, works_df, tag_vocab):\n",
        "        \"\"\"\n",
        "        works_df must contain a 'tag_name_list' column (list of valid tag names).\n",
        "        tag_vocab is the list of allowed tag names (strings).\n",
        "        \"\"\"\n",
        "        df = works_df.copy()\n",
        "        df[\"text_input\"] = self.build_input_text(df)\n",
        "\n",
        "        # MultiLabelBinarizer on allowed tags (ensures we never predict unknown tags)\n",
        "        self.mlb = MultiLabelBinarizer(classes=tag_vocab)\n",
        "        Y = self.mlb.fit_transform(df[\"tag_name_list\"])\n",
        "\n",
        "        # Simple TF-IDF + OneVsRest Logistic Regression\n",
        "        self.pipeline = Pipeline([\n",
        "            (\"tfidf\", TfidfVectorizer(max_features=20000, ngram_range=(1,2))),\n",
        "            (\"clf\", OneVsRestClassifier(LogisticRegression(max_iter=1000)))\n",
        "        ])\n",
        "        self.pipeline.fit(df[\"text_input\"], Y)\n",
        "        self.tag_index_to_name = {i: t for i, t in enumerate(self.mlb.classes_)}\n",
        "\n",
        "    def predict(self, works_df, threshold=0.5, top_k=None):\n",
        "        df = works_df.copy()\n",
        "        df[\"text_input\"] = self.build_input_text(df)\n",
        "        probs = self.pipeline.predict_proba(df[\"text_input\"])  # shape: (n_samples, n_labels)\n",
        "        predictions = []\n",
        "        for i, row_probs in enumerate(probs):\n",
        "            if top_k is not None:\n",
        "                # take top_k labels by probability\n",
        "                top_idx = np.argsort(row_probs)[::-1][:top_k]\n",
        "                picked = [self.tag_index_to_name[idx] for idx in top_idx if row_probs[idx] > 0]  # pick non-zero probabilities\n",
        "            else:\n",
        "                picked = [self.tag_index_to_name[idx] for idx, p in enumerate(row_probs) if p >= threshold]\n",
        "            predictions.append(picked)\n",
        "        return predictions\n",
        "\n",
        "    def evaluate(self, works_df, threshold=0.5):\n",
        "        df = works_df.copy()\n",
        "        df[\"text_input\"] = self.build_input_text(df)\n",
        "        Y_true = self.mlb.transform(df[\"tag_name_list\"])\n",
        "        probs = self.pipeline.predict_proba(df[\"text_input\"])\n",
        "        Y_pred = (probs >= threshold).astype(int)\n",
        "        micro_f1 = f1_score(Y_true, Y_pred, average=\"micro\", zero_division=0)\n",
        "        micro_precision = precision_score(Y_true, Y_pred, average=\"micro\", zero_division=0)\n",
        "        micro_recall = recall_score(Y_true, Y_pred, average=\"micro\", zero_division=0)\n",
        "        return {\"f1\": micro_f1, \"precision\": micro_precision, \"recall\": micro_recall}\n",
        "\n",
        "# ---------------------------\n",
        "# Gradio app functions\n",
        "# ---------------------------\n",
        "state = {\n",
        "    \"works_df\": None,\n",
        "    \"tags_df\": None,\n",
        "    \"tag_mapping\": None,\n",
        "    \"baseline\": TagPredictorBaseline(),\n",
        "    \"tag_vocab\": None\n",
        "}\n",
        "\n",
        "def upload_tags_file(file_obj):\n",
        "    try:\n",
        "        df = pd.read_csv(file_obj)\n",
        "    except Exception as e:\n",
        "        return f\"Failed to read tags CSV: {e}\", None\n",
        "    state[\"tags_df\"] = df\n",
        "    # auto-detect mapping\n",
        "    try:\n",
        "        mapping, id_col, name_col = build_tag_mapping(df)\n",
        "    except Exception as e:\n",
        "        return f\"Tags file loaded but mapping failed: {e}\", df.head().to_csv(index=False)\n",
        "    state[\"tag_mapping\"] = mapping\n",
        "    return f\"Tags file loaded. Detected id column '{id_col}', name column '{name_col}'. {len(mapping)} tags loaded.\", df.head().to_csv(index=False)\n",
        "\n",
        "def upload_works_file(file_obj):\n",
        "    try:\n",
        "        df = pd.read_csv(file_obj)\n",
        "    except Exception as e:\n",
        "        return f\"Failed to read works CSV: {e}\", None\n",
        "    # parse tag id strings into list\n",
        "    if \"tags\" in df.columns:\n",
        "        df[\"tag_id_list\"] = df[\"tags\"].apply(parse_tag_string)\n",
        "    else:\n",
        "        df[\"tag_id_list\"] = [[] for _ in range(len(df))]\n",
        "    state[\"works_df\"] = df\n",
        "    return f\"Works file loaded with {len(df)} rows.\", df.head().to_csv(index=False)\n",
        "\n",
        "def map_ids_to_names_preview():\n",
        "    if state[\"works_df\"] is None or state[\"tag_mapping\"] is None:\n",
        "        return \"Upload both works and tags files first.\", \"\"\n",
        "    df = state[\"works_df\"].copy()\n",
        "    df[\"tag_name_list\"] = df[\"tag_id_list\"].apply(lambda lst: ids_to_names(lst, state[\"tag_mapping\"]))\n",
        "    # convert back to csv string for preview\n",
        "    df[\"tags_comma_string\"] = df[\"tag_name_list\"].apply(names_list_to_csv_string)\n",
        "    state[\"works_df\"] = df  # store\n",
        "    # Also build tag vocab for the model (unique tag names seen in mapping)\n",
        "    tag_vocab = sorted(list(set([t for sub in df[\"tag_name_list\"].tolist() for t in sub])))\n",
        "    state[\"tag_vocab\"] = tag_vocab\n",
        "    return f\"Mapped tags. Found {len(tag_vocab)} unique tag names in works (that exist in tags file).\", df.head().to_csv(index=False)\n",
        "\n",
        "def quick_train_demo(test_size=0.2, random_state=42):\n",
        "    if state[\"works_df\"] is None or state[\"tag_vocab\"] is None:\n",
        "        return \"Map IDs to names first (press 'Map IDs → Names')\", \"\"\n",
        "    df = state[\"works_df\"].copy()\n",
        "    # require tag_name_list column\n",
        "    if \"tag_name_list\" not in df.columns:\n",
        "        return \"Map IDs to names column missing.\", \"\"\n",
        "    # filter out works with no tags (optional)\n",
        "    df_train = df.copy()\n",
        "    # quick split\n",
        "    train_df, val_df = train_test_split(df_train, test_size=test_size, random_state=random_state)\n",
        "    baseline = TagPredictorBaseline()\n",
        "    baseline.fit(train_df, state[\"tag_vocab\"])\n",
        "    state[\"baseline\"] = baseline\n",
        "    # evaluate\n",
        "    metrics = baseline.evaluate(val_df)\n",
        "    metrics_text = f\"Validation micro-F1: {metrics['f1']:.4f}, Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}\"\n",
        "    return \"Quick baseline trained.\", metrics_text\n",
        "\n",
        "def predict_for_uploaded(threshold=0.5, top_k=None):\n",
        "    if state[\"works_df\"] is None:\n",
        "        return \"No works uploaded.\", \"\"\n",
        "    baseline = state.get(\"baseline\", None)\n",
        "    if baseline is None or baseline.pipeline is None:\n",
        "        return \"Model not trained yet. Use 'Quick Train Demo' first.\", \"\"\n",
        "    df = state[\"works_df\"].copy()\n",
        "    preds = baseline.predict(df, threshold=threshold, top_k=top_k if top_k>0 else None)\n",
        "    df[\"predicted_tag_list\"] = preds\n",
        "    df[\"predicted_tags_csv\"] = df[\"predicted_tag_list\"].apply(names_list_to_csv_string)\n",
        "    # Show preview CSV\n",
        "    return f\"Predicted tags for {len(df)} works (preview below).\", df[[\"predicted_tags_csv\"]].head(50).to_csv(index=False)\n",
        "\n",
        "# ---------------------------\n",
        "# Gradio interface layout\n",
        "# ---------------------------\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## AO3 Tagging — Gradio prototype (Notebook)\\nUpload your `tags` CSV and `works` CSV (AO3-style tag IDs). This UI will parse, map IDs→names (ignoring unknown IDs), allow a quick baseline training, and produce predicted tag **names** as comma-separated strings.\")\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            tags_file = gr.File(label=\"Upload tags CSV (tags-20210226.csv)\", file_types=['.csv'])\n",
        "            tags_status = gr.Textbox(label=\"Tags status\", interactive=False)\n",
        "            tags_preview = gr.Textbox(label=\"Tags preview (CSV head)\", interactive=False)\n",
        "            upload_tags_btn = gr.Button(\"Load tags file\")\n",
        "        with gr.Column(scale=1):\n",
        "            works_file = gr.File(label=\"Upload works CSV (works-20210226.csv)\", file_types=['.csv'])\n",
        "            works_status = gr.Textbox(label=\"Works status\", interactive=False)\n",
        "            works_preview = gr.Textbox(label=\"Works preview (CSV head)\", interactive=False)\n",
        "            upload_works_btn = gr.Button(\"Load works file\")\n",
        "    gr.Markdown(\"---\")\n",
        "    map_btn = gr.Button(\"Map IDs → Names & Convert to comma-separated string\")\n",
        "    map_status = gr.Textbox(label=\"Mapping status\", interactive=False)\n",
        "    map_preview = gr.Textbox(label=\"Works with mapped tags preview\", interactive=False)\n",
        "    gr.Markdown(\"### Quick baseline training (fast demo)\")\n",
        "    with gr.Row():\n",
        "        train_btn = gr.Button(\"Quick Train Demo (TF-IDF + One-vs-Rest LR)\")\n",
        "        train_status = gr.Textbox(label=\"Training status\", interactive=False)\n",
        "        train_metrics = gr.Textbox(label=\"Validation metrics\", interactive=False)\n",
        "    gr.Markdown(\"### Predict\")\n",
        "    with gr.Row():\n",
        "        threshold_slider = gr.Slider(0.0, 1.0, value=0.5, step=0.01, label=\"Prediction threshold (sigmoid)\")\n",
        "        topk_input = gr.Number(value=0, label=\"Top-K (0 = use threshold)\", precision=0)\n",
        "        predict_btn = gr.Button(\"Predict tags for uploaded works\")\n",
        "        predict_status = gr.Textbox(label=\"Prediction status\", interactive=False)\n",
        "        predict_preview = gr.Textbox(label=\"Prediction preview (predicted_tags_csv column)\", interactive=False)\n",
        "\n",
        "    # Bind buttons to functions\n",
        "    upload_tags_btn.click(fn=lambda f: upload_tags_file(f.name) if f is not None else (\"No file provided.\", \"\"), inputs=[tags_file], outputs=[tags_status, tags_preview])\n",
        "    upload_works_btn.click(fn=lambda f: upload_works_file(f.name) if f is not None else (\"No file provided.\", \"\"), inputs=[works_file], outputs=[works_status, works_preview])\n",
        "    map_btn.click(fn=map_ids_to_names_preview, inputs=[], outputs=[map_status, map_preview])\n",
        "    train_btn.click(fn=quick_train_demo, inputs=[], outputs=[train_status, train_metrics])\n",
        "    predict_btn.click(fn=lambda th, k: predict_for_uploaded(threshold=th, top_k=int(k)), inputs=[threshold_slider, topk_input], outputs=[predict_status, predict_preview])\n",
        "\n",
        "# Launch the Gradio interface in notebook (visible inline)\n",
        "demo.launch(share=False, inbrowser=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "id": "QDmZ7lBshRCt",
        "outputId": "57f595a0-4e94-4195-aab0-4a80a164306f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7860, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    }
  ]
}